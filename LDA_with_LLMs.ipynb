{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahinutar/colabs/blob/main/LDA_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA and LLMs for Document Summarization"
      ],
      "metadata": {
        "id": "KYNx8LFq59Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary libraries not installed in colab."
      ],
      "metadata": {
        "id": "rqI8qrJc6PvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install tiktoken\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZO1J-US5fCT",
        "outputId": "3b07e09f-93c4-4a06-84e8-60234e5f0443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.0-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.0/276.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.285-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
            "  Downloading langsmith-0.0.35-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.285 langsmith-0.0.35 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnuarluthuAE"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from pypdf import PdfReader\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions\n",
        "\n",
        "The following are the functions that implement the prepocessing, topic extraction, and LLM call."
      ],
      "metadata": {
        "id": "06VdMKxu8Mie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text, stop_words):\n",
        "    \"\"\"\n",
        "    Tokenizes and preprocesses the input text, removing stopwords and short\n",
        "    tokens.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to preprocess.\n",
        "        stop_words (set): A set of stopwords to be removed from the text.\n",
        "    Returns:\n",
        "        list: A list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for token in simple_preprocess(text, deacc=True):\n",
        "        if token not in stop_words and len(token) > 3:\n",
        "            result.append(token)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "sfdeuTP3zlEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_topic_lists_from_pdf(file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Extracts topics and their associated words from a PDF document using the\n",
        "    Latent Dirichlet Allocation (LDA) algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        file (str): The path to the PDF file for topic extraction.\n",
        "        num_topics (int): The number of topics to discover.\n",
        "        words_per_topic (int): The number of words to include per topic.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of num_topics sublists, each containing relevant words\n",
        "        for a topic.\n",
        "    \"\"\"\n",
        "    # Load the pdf file\n",
        "    loader = PdfReader(file)\n",
        "\n",
        "    # Extract the text from each page into a list. Each page is considered a document\n",
        "    documents= []\n",
        "    for page in loader.pages:\n",
        "        documents.append(page.extract_text())\n",
        "\n",
        "    # Preprocess the documents\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words(['english','spanish']))\n",
        "    processed_documents = [preprocess(doc, stop_words) for doc in documents]\n",
        "\n",
        "    # Create a dictionary and a corpus\n",
        "    dictionary = corpora.Dictionary(processed_documents)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(\n",
        "        corpus,\n",
        "        num_topics=num_topics,\n",
        "        id2word=dictionary,\n",
        "        passes=15\n",
        "        )\n",
        "\n",
        "    # Retrieve the topics and their corresponding words\n",
        "    topics = lda_model.print_topics(num_words=words_per_topic)\n",
        "\n",
        "    # Store each list of words from each topic into a list\n",
        "    topics_ls = []\n",
        "    for topic in topics:\n",
        "        words = topic[1].split(\"+\")\n",
        "        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
        "        topics_ls.append(topic_words)\n",
        "\n",
        "    return topics_ls\n"
      ],
      "metadata": {
        "id": "8BXRZ_W1zmJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topics_from_pdf(llm, file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Generates descriptive prompts for LLM based on topic words extracted from a\n",
        "    PDF document.\n",
        "\n",
        "    This function takes the output of `get_topic_lists_from_pdf` function,\n",
        "    which consists of a list of topic-related words for each topic, and\n",
        "    generates an output string in bulleted nested list format.\n",
        "\n",
        "    Parameters:\n",
        "        llm (LLM): An instance of the Large Language Model (LLM) for generating\n",
        "        responses.\n",
        "        file (str): The path to the PDF file for extracting topic-related words.\n",
        "        num_topics (int): The number of topics to consider.\n",
        "        words_per_topic (int): The number of words per topic to include.\n",
        "\n",
        "    Returns:\n",
        "        str: A response generated by the language model based on the provided\n",
        "        topic words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract topics and convert them to string\n",
        "    list_of_topicwords = get_topic_lists_from_pdf(file, num_topics,\n",
        "                                                  words_per_topic)\n",
        "    string_lda = \"\"\n",
        "    for list in list_of_topicwords:\n",
        "        string_lda += str(list) + \"\\n\"\n",
        "\n",
        "    # Create the template\n",
        "    template_string = '''Describe the topic of each of the {num_topics}\n",
        "        double-quote delimited lists in a simple sentence and also write down\n",
        "        three possible different subthemes. The lists are the result of an\n",
        "        algorithm for topic discovery.\n",
        "        Do not provide an introduction or a conclusion, only describe the\n",
        "        topics. Do not mention the word \"topic\" when describing the topics.\n",
        "        Use the following template for the response.\n",
        "\n",
        "        1: <<<(sentence describing the topic)>>>\n",
        "        - <<<(Phrase describing the first subtheme)>>>\n",
        "        - <<<(Phrase describing the second subtheme)>>>\n",
        "        - <<<(Phrase describing the third subtheme)>>>\n",
        "\n",
        "        2: <<<(sentence describing the topic)>>>\n",
        "        - <<<(Phrase describing the first subtheme)>>>\n",
        "        - <<<(Phrase describing the second subtheme)>>>\n",
        "        - <<<(Phrase describing the third subtheme)>>>\n",
        "\n",
        "        ...\n",
        "\n",
        "        n: <<<(sentence describing the topic)>>>\n",
        "        - <<<(Phrase describing the first subtheme)>>>\n",
        "        - <<<(Phrase describing the second subtheme)>>>\n",
        "        - <<<(Phrase describing the third subtheme)>>>\n",
        "\n",
        "        Lists: \"\"\"{string_lda}\"\"\" '''\n",
        "\n",
        "    # LLM call\n",
        "    prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    response = chain.run({\n",
        "        \"string_lda\" : string_lda,\n",
        "        \"num_topics\" : num_topics\n",
        "        })\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "mhgTvRHQzmGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI API key\n",
        "\n",
        "For this demo, we are going to use chatgpt-3.5 Turbo. For that, it is necessary to introduce the API key. Check [How to get an OPEN API key for ChatGPT](https://www.maisieai.com/help/how-to-get-an-openai-api-key-for-chatgpt) for instructions on how to get one."
      ],
      "metadata": {
        "id": "_gIePRtyINYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key = \"sk-V...\"\n",
        "llm = OpenAI(openai_api_key=openai_key, max_tokens=-1)"
      ],
      "metadata": {
        "id": "zF-pM0cNIMqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing with documents\n",
        "\n",
        "Now, lets try with a public domain pdf document, The Metamorphosis By Franz Kafka (1915)."
      ],
      "metadata": {
        "id": "ypRlS4Ad8mkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://www.sigortta.com/static/data/assistance/file_15.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-QfH7Zm9Ci2",
        "outputId": "f4fc77cd-1076-469f-eba7-0565f5fbe805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mpXUmuLGzkVEqsTicQvBPcpPJW0aPqdL\n",
            "To: /content/the-metamorphosis.pdf\n",
            "\r  0% 0.00/427k [00:00<?, ?B/s]\r100% 427k/427k [00:00<00:00, 18.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"./the-metamorphosis.pdf\"\n",
        "\n",
        "num_topics = 6\n",
        "words_per_topic = 30\n",
        "\n",
        "summary = topics_from_pdf(llm, file, num_topics, words_per_topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUEsLb8t9Cc9",
        "outputId": "3d0da1ab-06c4-4c29-81ad-2a0635b55de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA9M5bG-9CTG",
        "outputId": "d8a922ed-4fd4-4fda-e641-d4af614bb7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1: The transformation of Gregor and its impact on his family\n",
            "- The physical changes undergone by Gregor\n",
            "- The psychological effects on his family\n",
            "- The impact of Gregor's transformation on his relationships\n",
            "\n",
            "2: The living space of Gregor and his family\n",
            "- The furniture in the room\n",
            "- The activities taking place in the living space\n",
            "- The symbolic meaning of the room\n",
            "\n",
            "3: The reactions of Gregor's family to his transformation\n",
            "- The different ways the family members react\n",
            "- The emotions and thoughts experienced by the family\n",
            "- The attempts of the family to cope with the changes\n",
            "\n",
            "4: The efforts of Gregor to adapt to his new state\n",
            "- Gregor's attempts to adjust to his new body\n",
            "- The challenges he faces in his new form\n",
            "- The support he receives from his family and society\n",
            "\n",
            "5: The daily routine of Gregor and his family\n",
            "- The mundane tasks Gregor performs\n",
            "- The activities of the family members\n",
            "- The changes in Gregor's life due to his transformation\n",
            "\n",
            "6: The relationship between Gregor and his family members\n",
            "- The role of Gregor in the family\n",
            "- The dynamics between Gregor and his family\n",
            "- The impact of Gregor's transformation on the family relationships\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, let's try with a technical book: The Foundations of Geometry by David Hilbert (1899).\n"
      ],
      "metadata": {
        "id": "SfRzqVI7yOQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1T_FeuGsoC08U_6Xb8Awt50CJXBUqji4D\n",
        "\n",
        "file = \"./Hilbert.pdf\"\n",
        "summary = topics_from_pdf(llm, file, num_topics, words_per_topic)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Do0XIP-yYBv",
        "outputId": "91d5836d-f972-4c53-f3a1-eef2e0b95c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1T_FeuGsoC08U_6Xb8Awt50CJXBUqji4D\n",
            "To: /content/Hilbert.pdf\n",
            "\r  0% 0.00/878k [00:00<?, ?B/s]\r100% 878k/878k [00:00<00:00, 26.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1: Analyzing geometric shapes, lines, and points\n",
            "- Point and line relationships\n",
            "- Congruent shapes\n",
            "- Geometric functions\n",
            "\n",
            "2: Applying axioms to geometric shapes\n",
            "- Parallel lines\n",
            "- Triangles and polygons\n",
            "- Algebraic equations\n",
            "\n",
            "3: Calculating area and content\n",
            "- Area of triangles and polygons\n",
            "- Measurement theory\n",
            "- Archimedes' theorem\n",
            "\n",
            "4: Describing congruence of shapes\n",
            "- Angle and side relationships\n",
            "- Congruence of triangles\n",
            "- Parallel lines and segments\n",
            "\n",
            "5: Exploring the multiplication of points, lines, and numbers\n",
            "- Laws of multiplication\n",
            "- Point and line order\n",
            "- Polygons and algebra\n",
            "\n",
            "6: Analyzing complex numbers and functions\n",
            "- Domain and range of a function\n",
            "- Jordan curve theorem\n",
            "- Reversible displacement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free experiment with the **number of topics** and the number of **words per topic** and find the combination that works for your document."
      ],
      "metadata": {
        "id": "4HP3wg7wN-pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Licence\n",
        "\n",
        "GNU General Public License v2.0"
      ],
      "metadata": {
        "id": "ZHrtCfay8ieJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Author\n",
        "\n",
        "[Antonio Jimenez](https://www.linkedin.com/in/antonio-jimnzc)"
      ],
      "metadata": {
        "id": "N5LxW1PgPTh1"
      }
    }
  ]
}